{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EAL7YbIqGFr"
      },
      "source": [
        "**Занятие первое**\n",
        "\n",
        "Начнем с простого. Многие знают что такое map и reduce операции, но все же для закрпеления мы их тут реализуем. Ах да, не забудем и про shuffle. Делать все будем на упрощенной задаче с word count для ознакомления с самим подходом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DBUYlacS6nb"
      },
      "source": [
        "На самом деле мы рассмптрим все в упрощенном виде, но это даст нам понимание, как можно через hadoop streaming, например, писать самописные map и reduce операции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHxuTfZ1TKc9"
      },
      "source": [
        "! mapred streaming \\\n",
        "  -input /wiki/sample.jsonl \\\n",
        "  -output /word-count \\\n",
        "  -mapper \"/opt/conda/bin/python3.6 mapper.py\" \\\n",
        "  -reducer \"/opt/conda/bin/python3.6 reducer.py\" \\\n",
        "  -file mapper.py \\\n",
        "  -file reducer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe2aSFz_Tgqv"
      },
      "source": [
        "Выше mapper.py и reducer.py это программы, которые выполняют одноименные операции нам потоком информации из jsonl файла, записывая ответ в файл word-count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hGAAKrdu5d-",
        "outputId": "0eb9e138-c6e7-4170-e100-cceae82d59d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re \n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPXzd-YMtqcO"
      },
      "source": [
        "Давайте загрузим файл с текстом и посмотрим на него"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKUCkYpBp9Lt"
      },
      "outputs": [],
      "source": [
        "with open('spark_text.txt', 'rb') as f:\n",
        "    data = f.readlines()\n",
        "data = [text.decode() for text in data if text.decode() != '\\r\\n']    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CywPTchftnqK",
        "outputId": "e6952263-6266-415d-9361-9a449fa3f5bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "0wRwGdoJ5pQs",
        "outputId": "74da6396-3a7e-4b24-a076-46d93d4537c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n'"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydjOU0HLwCZq"
      },
      "source": [
        "Как бы мы сделали..\n",
        "Надо немного почистить слова, а также сделать все в парадигме MapReduce. Понятно, что можно все написать проще, но мы ведь хотим понять, как это работает=)\n",
        "\n",
        "Загрузим стоп слова, очистим от них текст, приведем к нижнему регистру, всем раздадим ключи"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-zIUslxxtyQ"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words = set(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOXxYSKI2EfQ",
        "outputId": "007979aa-8205-49e1-f75f-462568ad3f00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDK6W4MUewdv"
      },
      "source": [
        "пунктуацию тоже полезно бы удалить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "XhrSCeUJ2MKZ",
        "outputId": "cfa3a9fd-00e8-4c2b-bc0b-fa446aa12d4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0AjYtsiv9tc"
      },
      "outputs": [],
      "source": [
        "def mapper_text(text):\n",
        "    clean_text = re.sub(rf\"[{string.punctuation}]\", \"\", text)\n",
        "    words = nltk.word_tokenize(clean_text)\n",
        "    words_with_value = [(word.lower(), 1) for word in words \n",
        "                        if word not in stop_words]\n",
        "    words_with_value = sorted(words_with_value, key=lambda x:x[0])\n",
        "    return words_with_value\n",
        "\n",
        "def create_chunks(shuffled_data):\n",
        "    result = {}\n",
        "    for idx, data in shuffled_data:\n",
        "        if idx in result:\n",
        "            result[idx].append(data)\n",
        "        else:\n",
        "            result[idx] = [data]\n",
        "    return list(result.items())\n",
        "\n",
        "def shuffle_text(mapper_result, n_nodes=5):\n",
        "    shuffled_data = []\n",
        "    for key, value in mapper_result:\n",
        "        shuffled_data.append((hash(key)%n_nodes, (key, value)))\n",
        "    shuffled_data = sorted(shuffled_data, key=lambda x: x[0])\n",
        "    chunks = create_chunks(shuffled_data)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# на самом деле для reduce в жизни пишут иначе..не зря мы сортируем внутри map\n",
        "#данные по ключам. Это нужно для избавления от этапа проверки ключа и поиска\n",
        "def reduce_text(values_to_reduce):\n",
        "    result = {}\n",
        "    for key, value in values_to_reduce:\n",
        "        if key in result:\n",
        "            result[key] += 1\n",
        "        else:\n",
        "            result[key] = 1\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7qztZ0ijcqf"
      },
      "source": [
        "Проверим, что все работает\n",
        "\n",
        "Сначала map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "133g98tQMT8I",
        "outputId": "cf08ffd4-edf6-4a86-d224-d7bb5a6c7dc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\\n'"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpqqPnNRhCXp"
      },
      "outputs": [],
      "source": [
        "map_stage = mapper_text(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hiOXuxSsDno",
        "outputId": "df8debdd-5a1b-44c0-b635-d9b29f52f99f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('amplab', 1),\n",
              " ('analytics', 1),\n",
              " ('apache', 1),\n",
              " ('apache', 1),\n",
              " ('berkeley', 1),\n",
              " ('california', 1),\n",
              " ('clusters', 1),\n",
              " ('codebase', 1),\n",
              " ('data', 1),\n",
              " ('data', 1),\n",
              " ('developed', 1),\n",
              " ('donated', 1),\n",
              " ('engine', 1),\n",
              " ('entire', 1),\n",
              " ('fault', 1),\n",
              " ('foundation', 1),\n",
              " ('implicit', 1),\n",
              " ('interface', 1),\n",
              " ('largescale', 1),\n",
              " ('later', 1),\n",
              " ('maintained', 1),\n",
              " ('opensource', 1),\n",
              " ('originally', 1),\n",
              " ('parallelism', 1),\n",
              " ('processing', 1),\n",
              " ('programming', 1),\n",
              " ('provides', 1),\n",
              " ('since', 1),\n",
              " ('software', 1),\n",
              " ('spark', 1),\n",
              " ('spark', 1),\n",
              " ('spark', 1),\n",
              " ('tolerance', 1),\n",
              " ('unified', 1),\n",
              " ('university', 1)]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "map_stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoOq2kxGl4FM"
      },
      "source": [
        "shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLyQcJ7Xjmll"
      },
      "outputs": [],
      "source": [
        "shuffle_stage = shuffle_text(map_stage, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig2vthBFsNTm",
        "outputId": "a1436c8d-52ef-48ef-df6d-fa5d0b4c48bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0,\n",
              "  [('berkeley', 1),\n",
              "   ('clusters', 1),\n",
              "   ('developed', 1),\n",
              "   ('foundation', 1),\n",
              "   ('implicit', 1),\n",
              "   ('interface', 1),\n",
              "   ('later', 1),\n",
              "   ('parallelism', 1),\n",
              "   ('unified', 1)]),\n",
              " (1,\n",
              "  [('amplab', 1),\n",
              "   ('analytics', 1),\n",
              "   ('california', 1),\n",
              "   ('data', 1),\n",
              "   ('data', 1),\n",
              "   ('engine', 1),\n",
              "   ('spark', 1),\n",
              "   ('spark', 1),\n",
              "   ('spark', 1),\n",
              "   ('university', 1)]),\n",
              " (2, [('apache', 1), ('apache', 1), ('donated', 1), ('processing', 1)]),\n",
              " (3,\n",
              "  [('entire', 1),\n",
              "   ('fault', 1),\n",
              "   ('largescale', 1),\n",
              "   ('opensource', 1),\n",
              "   ('programming', 1),\n",
              "   ('provides', 1),\n",
              "   ('software', 1)]),\n",
              " (4,\n",
              "  [('codebase', 1),\n",
              "   ('maintained', 1),\n",
              "   ('originally', 1),\n",
              "   ('since', 1),\n",
              "   ('tolerance', 1)])]"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shuffle_stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rqyNgzpl8q1"
      },
      "source": [
        "reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJed4iQsll9i",
        "outputId": "72da51e1-6987-407f-eaaf-7295ed766f14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'amplab': 1,\n",
              " 'analytics': 1,\n",
              " 'california': 1,\n",
              " 'data': 2,\n",
              " 'engine': 1,\n",
              " 'spark': 3,\n",
              " 'university': 1}"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reduce_text(shuffle_stage[1][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOuqRbx5srNh"
      },
      "source": [
        "Итак, осталось все рассчитать параллельно и собрать результаты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5NgoQwpasxwq"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3ogNEUhtvkD"
      },
      "outputs": [],
      "source": [
        "n_nodes = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rA9awfQ4RSo"
      },
      "source": [
        "Обернем в 1 функциию для удобства map и shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TItZtKEF4Qiu"
      },
      "outputs": [],
      "source": [
        "def map_shuffle(text, n_nodes):\n",
        "    map_result = mapper_text(text)\n",
        "    shuffle_result = shuffle_text(map_result, n_nodes)\n",
        "    return shuffle_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUvuGGNdro3U",
        "outputId": "127425e2-465d-451d-9d5b-4ac6a5200eb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.[2] The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged[3] even though the RDD API is not deprecated.[4][5] The RDD technology still underlies the Dataset API.[6][7]\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark RDDs \\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.[8]\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark facilitates the implementation of both iterative algorithm\\n', 5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('ms, which visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data. The latency of such applications may be reduced by \\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('several orders of magnitude compared to Apache Hadoop MapReduce implementation.[2][9] Among the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark.[10]\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Apache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster, where you can launch a cluster either manually or use the launch scripts provided by the install package. It is also possible to run these daemons on a single machine for testing), Hadoop YARN, Apache Mesos or \\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Kubernetes. [11] For distributed storage, Spark can interface with a wide variety, including Alluxio, Hadoop Distributed File System (HDFS),[12] MapR File System (MapR-FS),[13] Cassandra,[14] OpenStack Swift, Amazon S3, Kudu, Lustre file system,[15] or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic I/O functionalities, exposed through an application programming interface (for Java, Python, Scala, .NET[16] and R) centered on the RDD abstraction (the Java API is available for other JVM languages, but is also usable for some other non-JVM languages that can connect to the JVM, such as Julia[17]). \\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('This interface mirrors a functional/higher-order model of programming: a \"driver\" program invokes parallel operations such as map, filter or reduce on an RDD by passing a function to Spark, which then schedules the function\\'s execution in parallel on the cluster.[2] These operations, and additional ones such as joins, take RDDs as input and produce new RDDs. RDDs are immutable and their operations are lazy; fault-tolerance is achieved by keeping track of the \"lineage\" of each RDD (the sequence of operations that produced it) so that it can be reconstructed in the case of data loss. RDDs can contain any type of Python, .NET, Java, or Scala objects.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Besides the RDD-oriented functional style of programming, Spark provides two restricted forms of shared variables: broadcast variables reference read-only data that needs to be available on all nodes, while accumulators can be used to program reductions in an imperative style.[2]\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('A typical example of RDD-centric functional programming is the following Scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones. Each map, flatMap (a variant of map) and reduceByKey takes an anonymous function that performs a simple operation on a single data item (or a pair of items), and applies its argument to transform an RDD into a new RDD.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark SQL is a component on top of Spark Core that introduced a data abstraction called DataFrames,[a] which provides support for structured and semi-structured data. Spark SQL provides a domain-specific language (DSL) to manipulate DataFrames in Scala, Java, Python or .NET.[16] It also provides SQL language support, with command-line interfaces and ODBC/JDBC server. Although DataFrames lack the compile-time type-checking afforded by RDDs, as of Spark 2.0, the strongly typed DataSet is fully supported by Spark SQL as well.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  (\"Spark Streaming uses Spark Core's fast scheduling capability to perform streaming analytics. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, thus facilitating easy implementation of lambda architecture.[19][20] \\n\",\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('However, this convenience comes with the penalty of latency equal to the mini-batch duration. Other streaming data engines that process event by event rather than in mini-batches include Storm and the streaming component of Flink.[21] Spark Streaming has support built-in to consume from Kafka, Flume, Twitter, ZeroMQ, Kinesis, and TCP/IP sockets.[22]\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('In Spark 2.x, a separate technology based on Datasets, called Structured Streaming, that has a higher-level interface is also provided to support streaming.[23]\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark can be deployed in a traditional on-premises data center as well as in the cloud.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark MLlib is a distributed machine-learning framework on top of Spark Core that, due in large part to the distributed memory-based Spark architecture, is as much as nine times as fast as the disk-based implementation used by Apache Mahout (according to benchmarks done by the MLlib developers against the alternating least squares (ALS) implementations, and before Mahout itself gained a Spark interface), and scales better than Vowpal Wabbit.[24] Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning pipelines, including\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('GraphX is a distributed graph-processing framework on top of Apache Spark. \\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Because it is based on RDDs, which are immutable, graphs are immutable and thus GraphX is unsuitable for graphs that need to be updated, let alone in a transactional manner like a graph database.[26] GraphX provides two separate APIs for implementation of massively parallel algorithms (such as PageRank): a Pregel abstraction, and a more general MapReduce-style API.[27] Unlike its predecessor Bagel, which was formally deprecated in Spark 1.6, GraphX has full support for property graphs (graphs where properties can be attached to edges and vertices).[28]\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('GraphX can be viewed as being the Spark in-memory version of Apache Giraph, which utilized Hadoop disk-based MapReduce.[29]\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  (\"Like Apache Spark, GraphX initially started as a research project at UC Berkeley's AMPLab and Databricks, and was later donated to the Apache Software Foundation and the Spark project.\\n\",\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Apache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute data processing tasks across multiple computers, either on its own or in tandem with other distributed computing tools. These two qualities are key to the worlds of big data and machine learning, which require the marshalling of massive computing power to crunch through large data stores. Spark also takes some of the programming burdens of these tasks off the shoulders of developers with an easy-to-use API that abstracts away much of the grunt work of distributed computing and big data processing.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('From its humble beginnings in the AMPLab at U.C. Berkeley in 2009, Apache Spark has become one of the key big data distributed processing frameworks in the world. Spark can be deployed in a variety of ways, provides native bindings for the Java, Scala, Python, and R programming languages, and supports SQL, streaming data, machine learning, and graph processing. You will find it used by banks, telecommunications companies, games companies, governments, and all of the major tech giants such as Apple, Facebook, IBM, and Microsoft.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  (\"At a fundamental level, an Apache Spark application consists of two main components: a driver, which converts the user's code into multiple tasks that can be distributed across worker nodes, and executors, which run on those nodes and execute the tasks assigned to them. Some form of cluster manager is necessary to mediate between the two.\\n\",\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Out of the box, Spark can run in a standalone cluster mode that simply requires the Apache Spark framework and a JVM on each machine in your cluster. However, it is more likely you will want to take advantage of a more robust resource or cluster management system to take care of allocating workers on demand for you. In the enterprise, this will normally mean running on Hadoop YARN (this is how the Cloudera and Hortonworks distributions run Spark jobs), but Apache Spark can also run on Apache Mesos, Kubernetes, and Docker Swarm.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('If you seek a managed solution, then Apache Spark can be found as part of Amazon EMR, Google Cloud Dataproc, and Microsoft Azure HDInsight. Databricks, the company that employs the founders of Apache Spark, also offers the Databricks Unified Analytics Platform, which is a comprehensive managed service that offers Apache Spark clusters, streaming support, integrated web-based notebook development, and optimized cloud I/O performance over a standard Apache Spark distribution.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Apache Spark builds the user data processing commands into a Directed Acyclic Graph, or DAG. The DAG is Apache Spark scheduling layer; it determines what tasks are executed on what nodes and in what sequence.  \\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark vs. Hadoop: Why use Apache Spark?\\n', 5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('It is worth pointing out that Apache Spark vs. Apache Hadoop is a bit of a misnomer. You will find Spark included in most Hadoop distributions these days. But due to two big advantages, Spark has become the framework of choice when processing big data, overtaking the old MapReduce paradigm that brought Hadoop to prominence.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('[ Download CIO new Roadmap Report on 5G in the enterprise! ]\\n', 5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('The first advantage is speed. Spark in-memory data engine means that it can perform tasks up to one hundred times faster than MapReduce in certain situations, particularly when compared with multi-stage jobs that require the writing of state back out to disk between stages. In essence, MapReduce creates a two-stage execution graph consisting of data mapping and reducing, whereas Apache Spark DAG has multiple stages that can be distributed more efficiently. Even Apache Spark jobs where the data cannot be completely contained within memory tend to be around 10 times faster than their MapReduce counterpart.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('The second advantage is the developer-friendly Spark API. As important as Spark speedup is, one could argue that the friendliness of the Spark API is even more important.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('Spark Core\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('In comparison to MapReduce and other Apache Hadoop components, the Apache Spark API is very friendly to developers, hiding much of the complexity of a distributed processing engine behind simple method calls. The canonical example of this is how almost 50 lines of MapReduce code to count words in a document can be reduced to just a few lines of Apache Spark (here shown in Scala):\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('Spark RDD\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('At the heart of Apache Spark is the concept of the Resilient Distributed Dataset (RDD), a programming abstraction that represents an immutable collection of objects that can be split across a computing cluster. Operations on the RDDs can also be split across the cluster and executed in a parallel batch process, leading to fast and scalable parallel processing.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('RDDs can be created from simple text files, SQL databases, NoSQL stores (such as Cassandra and MongoDB), Amazon S3 buckets, and much more besides. Much of the Spark Core API is built on this RDD concept, enabling traditional map and reduce functionality, but also providing built-in support for joining data sets, filtering, sampling, and aggregation.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark runs in a distributed fashion by combining a driver core process that splits a Spark application into tasks and distributes them among many executor processes that do the work. These executors can be scaled up and down as required for the applications needs.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('Spark SQL\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Originally known as Shark, Spark SQL has become more and more important to the Apache Spark project. It is likely the interface most commonly used by today developers when creating applications. Spark SQL is focused on the processing of structured data, using a dataframe approach borrowed from R and Python (in Pandas). But as the name suggests, Spark SQL also provides a SQL2003-compliant interface for querying data, bringing the power of Apache Spark to analysts as well as developers.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Alongside standard SQL support, Spark SQL provides a standard interface for reading from and writing to other datastores including JSON, HDFS, Apache Hive, JDBC, Apache ORC, and Apache Parquet, all of which are supported out of the box. Other popular stores Apache Cassandra, MongoDB, Apache HBase, and many others can be used by pulling in separate connectors from the Spark Packages ecosystem.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Selecting some columns from a dataframe is as simple as this line:\\n', 5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Apache Spark also bundles libraries for applying machine learning and graph analysis techniques to data at scale. Spark MLlib includes a framework for creating machine learning pipelines, allowing for easy implementation of feature extraction, selections, and transformations on any structured dataset. MLlib comes with distributed implementations of clustering and classification algorithms such as k-means clustering and random forests that can be swapped in and out of custom pipelines with ease. Models can be trained by data scientists in Apache Spark using R or Python, saved using MLlib, and then imported into a Java-based or Scala-based pipeline for production use.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Note that while Spark MLlib covers basic machine learning including classification, regression, clustering, and filtering, it does not include facilities for modeling and training deep neural networks (for details see InfoWorld Spark MLlib review). However, Deep Learning Pipelines are in the works.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('Spark Graph\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark GraphX comes with a selection of distributed algorithms for processing graph structures including an implementation of Google PageRank. These algorithms use Spark Core RDD approach to modeling data; the GraphFrames package allows you to do graph operations on dataframes, including taking advantage of the Catalyst optimizer for graph queries.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark Streaming\\n', 5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark Streaming was an early addition to Apache Spark that helped it gain traction in environments that required real-time or near real-time processing. Previously, batch and stream processing in the world of Apache Hadoop were separate things. You would write MapReduce code for your batch processing needs and use something like Apache Storm for your real-time streaming requirements. This obviously leads to disparate codebases that need to be kept in sync for the application domain despite being based on completely different frameworks, requiring different resources, and involving different operational concerns for running them.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Spark Streaming extended the Apache Spark concept of batch processing into streaming by breaking the stream down into a continuous series of microbatches, which could then be manipulated using the Apache Spark API. In this way, code in batch and streaming operations can share (mostly) the same code, running on the same framework, thus reducing both developer and operator overhead. Everybody wins.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('A criticism of the Spark Streaming approach is that microbatching, in scenarios where a low-latency response to incoming data is required, may not be able to match the performance of other streaming-capable frameworks like Apache Storm, Apache Flink, and Apache Apex, all of which use a pure streaming method rather than microbatches.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Structured Streaming\\n', 5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Structured Streaming (added in Spark 2.x) is to Spark Streaming what Spark SQL was to the Spark Core APIs: A higher-level API and easier abstraction for writing applications. In the case of Structure Streaming, the higher-level API essentially allows developers to create infinite streaming dataframes and datasets. It also solves some very real pain points that users have struggled with in the earlier framework, especially concerning dealing with event-time aggregations and late delivery of messages. All queries on structured streams go through the Catalyst query optimizer, and can even be run in an interactive manner, allowing users to perform SQL queries against live streaming data.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Structured Streaming originally relied on Spark Streaming microbatching scheme of handling streaming data. But in Spark 2.3, the Apache Spark team added a low-latency Continuous Processing Mode to Structured Streaming, allowing it to handle responses with latencies as low as 1ms, which is very impressive. As of Spark 2.4, Continuous Processing is still considered experimental. While Structured Streaming is built on top of the Spark SQL engine, Continuous Streaming supports only a restricted set of queries.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Structured Streaming is the future of streaming applications with the platform, so if you are building a new streaming application, you should use Structured Streaming. The legacy Spark Streaming APIs will continue to be supported, but the project recommends porting over to Structured Streaming, as the new method makes writing and maintaining streaming code a lot more bearable.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Deep Learning Pipelines\\n', 5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Apache Spark supports deep learning via Deep Learning Pipelines. Using the existing pipeline structure of MLlib, you can call into lower-level deep learning libraries and construct classifiers in just a few lines of code, as well as apply custom TensorFlow graphs or Keras models to incoming data. These graphs and models can even be registered as custom Spark SQL UDFs (user-defined functions) so that the deep learning models can be applied to data as part of SQL statements.\\n',\n",
              "   5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>, ('\\n', 5), {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Apache Spark tutorials\\n', 5),\n",
              "  {}),\n",
              " (<function __main__.map_shuffle(text, n_nodes)>,\n",
              "  ('Ready to dive in and learn Apache Spark? We highly recommend Evan Heitman A Neanderthal Guide to Apache Spark in Python, which not only lays out the basics of how Apache Spark works in relatively simple terms, but also guides you through the process of writing a simple Python application that makes use of the framework. The article is written from a data scientist perspective, which makes sense as data science is a world in which big data and machine learning are increasingly critical.\\n',\n",
              "   5),\n",
              "  {})]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[delayed(map_shuffle)(df, n_nodes) for df in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WOywHRItFWD",
        "outputId": "ecaa9062-0ad2-4ce0-a3f7-abf0d4c4a774"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   6 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=5)]: Done  25 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=5)]: Done  50 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=5)]: Done  85 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=5)]: Done  92 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    2.3s finished\n"
          ]
        }
      ],
      "source": [
        "with Parallel(n_jobs=n_nodes, verbose=10, batch_size=5) as parallel:\n",
        "    res = parallel(delayed(map_shuffle)(df, n_nodes) for df in data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIfatYoXMjPW",
        "outputId": "015f407a-8909-44fa-cc49-b15b019c1eed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EgpGGu9MknD",
        "outputId": "c9f7073a-5c00-4503-f619-dbf87112f925"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0,\n",
              "  [('analytics', 1),\n",
              "   ('california', 1),\n",
              "   ('donated', 1),\n",
              "   ('implicit', 1),\n",
              "   ('interface', 1),\n",
              "   ('originally', 1)]),\n",
              " (1,\n",
              "  [('software', 1),\n",
              "   ('spark', 1),\n",
              "   ('spark', 1),\n",
              "   ('spark', 1),\n",
              "   ('university', 1)]),\n",
              " (2, [('berkeley', 1), ('maintained', 1), ('unified', 1)]),\n",
              " (3,\n",
              "  [('amplab', 1),\n",
              "   ('apache', 1),\n",
              "   ('apache', 1),\n",
              "   ('data', 1),\n",
              "   ('data', 1),\n",
              "   ('developed', 1),\n",
              "   ('engine', 1),\n",
              "   ('programming', 1)]),\n",
              " (4,\n",
              "  [('clusters', 1),\n",
              "   ('codebase', 1),\n",
              "   ('entire', 1),\n",
              "   ('fault', 1),\n",
              "   ('foundation', 1),\n",
              "   ('largescale', 1),\n",
              "   ('later', 1),\n",
              "   ('opensource', 1),\n",
              "   ('parallelism', 1),\n",
              "   ('processing', 1),\n",
              "   ('provides', 1),\n",
              "   ('since', 1),\n",
              "   ('tolerance', 1)])]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvAYM5vA9K8N"
      },
      "source": [
        "Сделаем что-то вроде перессылки, собирая все в словари и заодно посмотрим на сколько равномерно распределлиись наши слова"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kTLnBO24013"
      },
      "outputs": [],
      "source": [
        "shuffle_stage = {i:[] for i in range(5)}\n",
        "for values in res:\n",
        "    values = dict(values)\n",
        "    for key in values.keys():\n",
        "        shuffle_stage[key].extend(values[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9sAjyP46kUE",
        "outputId": "f9310ac8-0a73-4e5c-f594-a1670b71893c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: number of words = 368\n",
            "1: number of words = 349\n",
            "2: number of words = 384\n",
            "3: number of words = 405\n",
            "4: number of words = 415\n"
          ]
        }
      ],
      "source": [
        "for key in shuffle_stage.keys():\n",
        "    print(f'{key}: number of words = {len(shuffle_stage[key])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVEA-LZG9eEv"
      },
      "source": [
        "И последний этап - нужно сделать reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i95b8FbDboX",
        "outputId": "9c3b897f-4e05-430b-cf68-4d0861a92497"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
          ]
        }
      ],
      "source": [
        "with Parallel(n_jobs=n_nodes, verbose=10, batch_size=5) as parallel:\n",
        "    res = parallel(delayed(reduce_text)(shuffle_stage[key]) for key in shuffle_stage.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkGsGeKE7FeS",
        "outputId": "50bb92f6-8db8-426f-8a02-6a2731163adc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8on05jYEMxUp",
        "outputId": "f0e4c915-3907-4e33-da52-f34e236ece3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'analytics': 1,\n",
              " 'california': 1,\n",
              " 'donated': 1,\n",
              " 'implicit': 1,\n",
              " 'interface': 5,\n",
              " 'originally': 2,\n",
              " 'api': 8,\n",
              " 'deprecated45': 1,\n",
              " 'encouraged3': 1,\n",
              " 'maintained': 1,\n",
              " 'programming': 8,\n",
              " 'spark': 34,\n",
              " 'still': 2,\n",
              " 'underlies': 1,\n",
              " 'way2': 1,\n",
              " 'cluster': 1,\n",
              " 'read': 1,\n",
              " 'reduce': 1,\n",
              " 'results': 2,\n",
              " 'store': 1,\n",
              " 'memory8': 1,\n",
              " 'offers': 3,\n",
              " 'working': 1,\n",
              " 'analysis': 1,\n",
              " 'visit': 1,\n",
              " 'among': 1,\n",
              " 'apache': 12,\n",
              " 'developing': 1,\n",
              " 'formed': 1,\n",
              " 'training': 1,\n",
              " 'run': 1,\n",
              " 'scripts': 1,\n",
              " 'testing': 1,\n",
              " 'alluxio': 1,\n",
              " 'hadoop': 6,\n",
              " 'local': 2,\n",
              " 'machine': 3,\n",
              " 'mode': 2,\n",
              " 'openstack': 1,\n",
              " 'supports': 3,\n",
              " 'swift': 1,\n",
              " 'centered': 1,\n",
              " 'dispatching': 1,\n",
              " 'exposed': 1,\n",
              " 'jvm': 2,\n",
              " 'overall': 1,\n",
              " 'task': 1,\n",
              " 'case': 1,\n",
              " 'contain': 1,\n",
              " 'filter': 1,\n",
              " 'immutable': 2,\n",
              " 'new': 1,\n",
              " 'objects': 2,\n",
              " 'rdds': 7,\n",
              " 'sequence': 2,\n",
              " 'this': 2,\n",
              " 'accumulators': 1,\n",
              " 'rddoriented': 1,\n",
              " 'readonly': 1,\n",
              " 'reference': 1,\n",
              " 'shared': 1,\n",
              " 'variables': 2,\n",
              " 'each': 1,\n",
              " 'following': 1,\n",
              " 'frequencies': 1,\n",
              " 'takes': 2,\n",
              " 'variant': 1,\n",
              " 'words': 1,\n",
              " 'abstraction': 1,\n",
              " 'dsl': 1,\n",
              " 'interfaces': 1,\n",
              " 'sql': 6,\n",
              " 'typechecking': 1,\n",
              " 'typed': 1,\n",
              " 'well': 3,\n",
              " 'batch': 1,\n",
              " 'capability': 1,\n",
              " 'design': 1,\n",
              " 'lambda': 1,\n",
              " 'written': 1,\n",
              " 'comes': 2,\n",
              " 'engines': 1,\n",
              " 'other': 1,\n",
              " 'zeromq': 1,\n",
              " 'higherlevel': 3,\n",
              " 'als': 1,\n",
              " 'architecture': 1,\n",
              " 'common': 1,\n",
              " 'distributed': 6,\n",
              " 'implemented': 1,\n",
              " 'machinelearning': 1,\n",
              " 'mllib': 3,\n",
              " 'used': 1,\n",
              " 'vowpal': 1,\n",
              " '16': 1,\n",
              " 'attached': 1,\n",
              " 'database26': 1,\n",
              " 'graph': 1,\n",
              " 'graphs': 6,\n",
              " 'graphx': 5,\n",
              " 'let': 1,\n",
              " 'properties': 1,\n",
              " 'diskbased': 1,\n",
              " 'giraph': 1,\n",
              " 'version': 1,\n",
              " 'amplab': 1,\n",
              " 'away': 1,\n",
              " 'distribute': 1,\n",
              " 'either': 1,\n",
              " 'framework': 4,\n",
              " 'multiple': 1,\n",
              " 'qualities': 1,\n",
              " 'tools': 1,\n",
              " 'apple': 1,\n",
              " 'become': 3,\n",
              " 'beginnings': 1,\n",
              " 'berkeley': 1,\n",
              " 'facebook': 1,\n",
              " 'consists': 1,\n",
              " 'level': 1,\n",
              " 'necessary': 1,\n",
              " 'docker': 1,\n",
              " 'robust': 1,\n",
              " 'simply': 1,\n",
              " 'swarm': 1,\n",
              " 'azure': 1,\n",
              " 'company': 1,\n",
              " 'comprehensive': 1,\n",
              " 'dataproc': 1,\n",
              " 'distribution': 1,\n",
              " 'emr': 1,\n",
              " 'found': 1,\n",
              " 'managed': 2,\n",
              " 'optimized': 1,\n",
              " 'platform': 1,\n",
              " 'seek': 1,\n",
              " 'service': 1,\n",
              " 'builds': 1,\n",
              " 'determines': 1,\n",
              " 'directed': 1,\n",
              " 'nodes': 1,\n",
              " 'vs': 2,\n",
              " 'bit': 1,\n",
              " 'worth': 1,\n",
              " 'around': 1,\n",
              " 'certain': 1,\n",
              " 'counterpart': 1,\n",
              " 'engine': 2,\n",
              " 'whereas': 1,\n",
              " 'writing': 2,\n",
              " 'canonical': 1,\n",
              " 'comparison': 1,\n",
              " 'complexity': 1,\n",
              " 'document': 1,\n",
              " 'rdd': 1,\n",
              " 'fast': 1,\n",
              " 'heart': 1,\n",
              " 'leading': 1,\n",
              " 'scalable': 1,\n",
              " 'besides': 1,\n",
              " 'built': 2,\n",
              " 'enabling': 1,\n",
              " 'joining': 1,\n",
              " 'sampling': 1,\n",
              " 'text': 1,\n",
              " 'applications': 1,\n",
              " 'needs': 2,\n",
              " 'analysts': 1,\n",
              " 'power': 1,\n",
              " 'shark': 1,\n",
              " 'sql2003compliant': 1,\n",
              " 'structured': 8,\n",
              " 'box': 1,\n",
              " 'hdfs': 1,\n",
              " 'hive': 1,\n",
              " 'pulling': 1,\n",
              " 'columns': 1,\n",
              " 'allowing': 2,\n",
              " 'clustering': 3,\n",
              " 'swapped': 1,\n",
              " 'covers': 1,\n",
              " 'however': 1,\n",
              " 'algorithms': 2,\n",
              " 'google': 1,\n",
              " 'queries': 2,\n",
              " 'based': 1,\n",
              " 'codebases': 1,\n",
              " 'gain': 1,\n",
              " 'like': 1,\n",
              " 'near': 1,\n",
              " 'need': 1,\n",
              " 'requirements': 1,\n",
              " 'would': 1,\n",
              " 'continuous': 1,\n",
              " 'operations': 1,\n",
              " 'overhead': 1,\n",
              " 'running': 1,\n",
              " 'stream': 1,\n",
              " 'approach': 1,\n",
              " 'incoming': 2,\n",
              " 'microbatches': 1,\n",
              " 'streamingcapable': 1,\n",
              " 'easier': 1,\n",
              " 'go': 1,\n",
              " 'late': 1,\n",
              " 'manner': 1,\n",
              " 'solves': 1,\n",
              " 'handling': 1,\n",
              " 'impressive': 1,\n",
              " 'set': 1,\n",
              " 'future': 1,\n",
              " 'learning': 6,\n",
              " 'pipelines': 2,\n",
              " 'applied': 1,\n",
              " 'classifiers': 1,\n",
              " 'existing': 1,\n",
              " 'part': 1,\n",
              " 'registered': 1,\n",
              " 'userdefined': 1,\n",
              " 'basics': 1,\n",
              " 'dive': 1}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZsgvg9oEVwt"
      },
      "source": [
        "Собираем результат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVKdO8eAD24r"
      },
      "outputs": [],
      "source": [
        "result = {}\n",
        "for partition in res:\n",
        "    for key in partition.keys():\n",
        "        if key in result:\n",
        "            result[key] += partition[key]\n",
        "        else:\n",
        "            result[key] = partition[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUTsOJf7EtW1",
        "outputId": "067818de-4a94-4037-b4ab-ea522f5ff037"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('spark', 110),\n",
              " ('apache', 56),\n",
              " ('data', 44),\n",
              " ('streaming', 36),\n",
              " ('distributed', 23),\n",
              " ('processing', 19),\n",
              " ('sql', 17),\n",
              " ('rdd', 15),\n",
              " ('learning', 15),\n",
              " ('also', 15),\n",
              " ('api', 14),\n",
              " ('structured', 13),\n",
              " ('cluster', 12),\n",
              " ('machine', 12),\n",
              " ('hadoop', 11),\n",
              " ('interface', 10),\n",
              " ('rdds', 10),\n",
              " ('mapreduce', 10),\n",
              " ('provides', 10),\n",
              " ('programming', 9),\n",
              " ('mllib', 9),\n",
              " ('graph', 9),\n",
              " ('framework', 9),\n",
              " ('the', 9),\n",
              " ('use', 9),\n",
              " ('core', 9),\n",
              " ('used', 8),\n",
              " ('support', 8),\n",
              " ('application', 8),\n",
              " ('python', 8),\n",
              " ('code', 8),\n",
              " ('tasks', 8),\n",
              " ('run', 7),\n",
              " ('graphx', 7),\n",
              " ('algorithms', 7),\n",
              " ('operations', 7),\n",
              " ('deep', 7),\n",
              " ('it', 7),\n",
              " ('dataset', 7),\n",
              " ('in', 7),\n",
              " ('abstraction', 6),\n",
              " ('batch', 6),\n",
              " ('graphs', 6),\n",
              " ('pipelines', 6),\n",
              " ('including', 6),\n",
              " ('two', 6),\n",
              " ('developers', 6),\n",
              " ('big', 6),\n",
              " ('scala', 6),\n",
              " ('map', 6),\n",
              " ('simple', 6),\n",
              " ('implementation', 6),\n",
              " ('analytics', 5),\n",
              " ('supports', 5),\n",
              " ('new', 5),\n",
              " ('writing', 5),\n",
              " ('applications', 5),\n",
              " ('set', 5),\n",
              " ('even', 5),\n",
              " ('top', 5),\n",
              " ('java', 5),\n",
              " ('parallel', 5),\n",
              " ('system', 5),\n",
              " ('computing', 5),\n",
              " ('project', 5),\n",
              " ('much', 5),\n",
              " ('using', 5),\n",
              " ('across', 5),\n",
              " ('these', 5),\n",
              " ('immutable', 4),\n",
              " ('well', 4),\n",
              " ('multiple', 4),\n",
              " ('nodes', 4),\n",
              " ('engine', 4),\n",
              " ('queries', 4),\n",
              " ('like', 4),\n",
              " ('continuous', 4),\n",
              " ('function', 4),\n",
              " ('times', 4),\n",
              " ('separate', 4),\n",
              " ('large', 4),\n",
              " ('advantage', 4),\n",
              " ('process', 4),\n",
              " ('custom', 4),\n",
              " ('models', 4),\n",
              " ('dataframes', 4),\n",
              " ('file', 4),\n",
              " ('required', 4),\n",
              " ('r', 4),\n",
              " ('a', 4),\n",
              " ('perform', 4),\n",
              " ('foundation', 4),\n",
              " ('one', 4),\n",
              " ('originally', 3),\n",
              " ('reduce', 3),\n",
              " ('offers', 3),\n",
              " ('mode', 3),\n",
              " ('jvm', 3),\n",
              " ('this', 3),\n",
              " ('comes', 3),\n",
              " ('higherlevel', 3),\n",
              " ('amplab', 3),\n",
              " ('become', 3),\n",
              " ('fast', 3),\n",
              " ('needs', 3),\n",
              " ('allowing', 3),\n",
              " ('clustering', 3),\n",
              " ('however', 3),\n",
              " ('based', 3),\n",
              " ('running', 3),\n",
              " ('approach', 3),\n",
              " ('part', 3),\n",
              " ('programs', 3),\n",
              " ('single', 3),\n",
              " ('take', 3),\n",
              " ('storm', 3),\n",
              " ('frameworks', 3),\n",
              " ('dag', 3),\n",
              " ('disk', 3),\n",
              " ('important', 3),\n",
              " ('concept', 3),\n",
              " ('dataframe', 3),\n",
              " ('thus', 3),\n",
              " ('apis', 3),\n",
              " ('structure', 3),\n",
              " ('supported', 3),\n",
              " ('scheduling', 3),\n",
              " ('databricks', 3),\n",
              " ('lines', 3),\n",
              " ('method', 3),\n",
              " ('many', 3),\n",
              " ('stores', 3),\n",
              " ('different', 3),\n",
              " ('2x', 3),\n",
              " ('restricted', 3),\n",
              " ('world', 3),\n",
              " ('languages', 3),\n",
              " ('program', 3),\n",
              " ('you', 3),\n",
              " ('users', 3),\n",
              " ('jobs', 3),\n",
              " ('standard', 3),\n",
              " ('makes', 3),\n",
              " ('storage', 3),\n",
              " ('amazon', 3),\n",
              " ('driver', 3),\n",
              " ('minibatches', 3),\n",
              " ('cloud', 3),\n",
              " ('but', 3),\n",
              " ('realtime', 3),\n",
              " ('donated', 2),\n",
              " ('maintained', 2),\n",
              " ('still', 2),\n",
              " ('results', 2),\n",
              " ('analysis', 2),\n",
              " ('among', 2),\n",
              " ('training', 2),\n",
              " ('testing', 2),\n",
              " ('local', 2),\n",
              " ('case', 2),\n",
              " ('objects', 2),\n",
              " ('sequence', 2),\n",
              " ('readonly', 2),\n",
              " ('shared', 2),\n",
              " ('variables', 2),\n",
              " ('takes', 2),\n",
              " ('words', 2),\n",
              " ('written', 2),\n",
              " ('other', 2),\n",
              " ('common', 2),\n",
              " ('implemented', 2),\n",
              " ('diskbased', 2),\n",
              " ('either', 2),\n",
              " ('berkeley', 2),\n",
              " ('managed', 2),\n",
              " ('platform', 2),\n",
              " ('vs', 2),\n",
              " ('besides', 2),\n",
              " ('built', 2),\n",
              " ('text', 2),\n",
              " ('power', 2),\n",
              " ('box', 2),\n",
              " ('google', 2),\n",
              " ('need', 2),\n",
              " ('stream', 2),\n",
              " ('incoming', 2),\n",
              " ('microbatches', 2),\n",
              " ('manner', 2),\n",
              " ('software', 2),\n",
              " ('resilient', 2),\n",
              " ('input', 2),\n",
              " ('response', 2),\n",
              " ('iterative', 2),\n",
              " ('latency', 2),\n",
              " ('reduced', 2),\n",
              " ('compared', 2),\n",
              " ('launch', 2),\n",
              " ('provided', 2),\n",
              " ('standalone', 2),\n",
              " ('executor', 2),\n",
              " ('for', 2),\n",
              " ('s3', 2),\n",
              " ('available', 2),\n",
              " ('basic', 2),\n",
              " ('io', 2),\n",
              " ('execution', 2),\n",
              " ('functions', 2),\n",
              " ('example', 2),\n",
              " ('language', 2),\n",
              " ('net16', 2),\n",
              " ('datasets', 2),\n",
              " ('technology', 2),\n",
              " ('traditional', 2),\n",
              " ('uc', 2),\n",
              " ('work', 2),\n",
              " ('find', 2),\n",
              " ('at', 2),\n",
              " ('enterprise', 2),\n",
              " ('management', 2),\n",
              " ('requires', 2),\n",
              " ('microsoft', 2),\n",
              " ('solution', 2),\n",
              " ('executed', 2),\n",
              " ('distributions', 2),\n",
              " ('faster', 2),\n",
              " ('inmemory', 2),\n",
              " ('reducing', 2),\n",
              " ('split', 2),\n",
              " ('cassandra', 2),\n",
              " ('files', 2),\n",
              " ('transformations', 2),\n",
              " ('could', 2),\n",
              " ('lowlatency', 2),\n",
              " ('added', 2),\n",
              " ('as', 2),\n",
              " ('unified', 2),\n",
              " ('items', 2),\n",
              " ('paradigm', 2),\n",
              " ('may', 2),\n",
              " ('manager', 2),\n",
              " ('native', 2),\n",
              " ('functional', 2),\n",
              " ('component', 2),\n",
              " ('event', 2),\n",
              " ('include', 2),\n",
              " ('called', 2),\n",
              " ('due', 2),\n",
              " ('pagerank', 2),\n",
              " ('companies', 2),\n",
              " ('components', 2),\n",
              " ('executors', 2),\n",
              " ('likely', 2),\n",
              " ('mesos', 2),\n",
              " ('builtin', 2),\n",
              " ('creating', 2),\n",
              " ('mongodb', 2),\n",
              " ('classification', 2),\n",
              " ('implementations', 2),\n",
              " ('catalyst', 2),\n",
              " ('package', 2),\n",
              " ('allows', 2),\n",
              " ('libraries', 2),\n",
              " ('pipeline', 2),\n",
              " ('developed', 2),\n",
              " ('form', 2),\n",
              " ('querying', 2),\n",
              " ('yarn', 2),\n",
              " ('kubernetes', 2),\n",
              " ('ones', 2),\n",
              " ('performs', 2),\n",
              " ('rather', 2),\n",
              " ('deployed', 2),\n",
              " ('mahout', 2),\n",
              " ('key', 2),\n",
              " ('require', 2),\n",
              " ('sets', 2),\n",
              " ('clusters', 2),\n",
              " ('completely', 2),\n",
              " ('stages', 2),\n",
              " ('filtering', 2),\n",
              " ('modeling', 2),\n",
              " ('microbatching', 2),\n",
              " ('optimizer', 2),\n",
              " ('later', 2),\n",
              " ('development', 2),\n",
              " ('variety', 2),\n",
              " ('easy', 2),\n",
              " ('scale', 2),\n",
              " ('performance', 2),\n",
              " ('works', 2),\n",
              " ('california', 1),\n",
              " ('implicit', 1),\n",
              " ('deprecated45', 1),\n",
              " ('encouraged3', 1),\n",
              " ('underlies', 1),\n",
              " ('way2', 1),\n",
              " ('read', 1),\n",
              " ('store', 1),\n",
              " ('memory8', 1),\n",
              " ('working', 1),\n",
              " ('visit', 1),\n",
              " ('developing', 1),\n",
              " ('formed', 1),\n",
              " ('scripts', 1),\n",
              " ('alluxio', 1),\n",
              " ('openstack', 1),\n",
              " ('swift', 1),\n",
              " ('centered', 1),\n",
              " ('dispatching', 1),\n",
              " ('exposed', 1),\n",
              " ('overall', 1),\n",
              " ('task', 1),\n",
              " ('contain', 1),\n",
              " ('filter', 1),\n",
              " ('accumulators', 1),\n",
              " ('rddoriented', 1),\n",
              " ('reference', 1),\n",
              " ('each', 1),\n",
              " ('following', 1),\n",
              " ('frequencies', 1),\n",
              " ('variant', 1),\n",
              " ('dsl', 1),\n",
              " ('interfaces', 1),\n",
              " ('typechecking', 1),\n",
              " ('typed', 1),\n",
              " ('capability', 1),\n",
              " ('design', 1),\n",
              " ('lambda', 1),\n",
              " ('engines', 1),\n",
              " ('zeromq', 1),\n",
              " ('als', 1),\n",
              " ('architecture', 1),\n",
              " ('machinelearning', 1),\n",
              " ('vowpal', 1),\n",
              " ('16', 1),\n",
              " ('attached', 1),\n",
              " ('database26', 1),\n",
              " ('let', 1),\n",
              " ('properties', 1),\n",
              " ('giraph', 1),\n",
              " ('version', 1),\n",
              " ('away', 1),\n",
              " ('distribute', 1),\n",
              " ('qualities', 1),\n",
              " ('tools', 1),\n",
              " ('apple', 1),\n",
              " ('beginnings', 1),\n",
              " ('facebook', 1),\n",
              " ('consists', 1),\n",
              " ('level', 1),\n",
              " ('necessary', 1),\n",
              " ('docker', 1),\n",
              " ('robust', 1),\n",
              " ('simply', 1),\n",
              " ('swarm', 1),\n",
              " ('azure', 1),\n",
              " ('company', 1),\n",
              " ('comprehensive', 1),\n",
              " ('dataproc', 1),\n",
              " ('distribution', 1),\n",
              " ('emr', 1),\n",
              " ('found', 1),\n",
              " ('optimized', 1),\n",
              " ('seek', 1),\n",
              " ('service', 1),\n",
              " ('builds', 1),\n",
              " ('determines', 1),\n",
              " ('directed', 1),\n",
              " ('bit', 1),\n",
              " ('worth', 1),\n",
              " ('around', 1),\n",
              " ('certain', 1),\n",
              " ('counterpart', 1),\n",
              " ('whereas', 1),\n",
              " ('canonical', 1),\n",
              " ('comparison', 1),\n",
              " ('complexity', 1),\n",
              " ('document', 1),\n",
              " ('heart', 1),\n",
              " ('leading', 1),\n",
              " ('scalable', 1),\n",
              " ('enabling', 1),\n",
              " ('joining', 1),\n",
              " ('sampling', 1),\n",
              " ('analysts', 1),\n",
              " ('shark', 1),\n",
              " ('sql2003compliant', 1),\n",
              " ('hdfs', 1),\n",
              " ('hive', 1),\n",
              " ('pulling', 1),\n",
              " ('columns', 1),\n",
              " ('swapped', 1),\n",
              " ('covers', 1),\n",
              " ('codebases', 1),\n",
              " ('gain', 1),\n",
              " ('near', 1),\n",
              " ('requirements', 1),\n",
              " ('would', 1),\n",
              " ('overhead', 1),\n",
              " ('streamingcapable', 1),\n",
              " ('easier', 1),\n",
              " ('go', 1),\n",
              " ('late', 1),\n",
              " ('solves', 1),\n",
              " ('handling', 1),\n",
              " ('impressive', 1),\n",
              " ('future', 1),\n",
              " ('applied', 1),\n",
              " ('classifiers', 1),\n",
              " ('existing', 1),\n",
              " ('registered', 1),\n",
              " ('userdefined', 1),\n",
              " ('basics', 1),\n",
              " ('dive', 1),\n",
              " ('university', 1),\n",
              " ('primary', 1),\n",
              " ('forces', 1),\n",
              " ('particular', 1),\n",
              " ('ie', 1),\n",
              " ('impetus', 1),\n",
              " ('initial', 1),\n",
              " ('spark10', 1),\n",
              " ('manually', 1),\n",
              " ('11', 1),\n",
              " ('instead', 1),\n",
              " ('system15', 1),\n",
              " ('connect', 1),\n",
              " ('functionalities', 1),\n",
              " ('nonjvm', 1),\n",
              " ('additional', 1),\n",
              " ('lazy', 1),\n",
              " ('lineage', 1),\n",
              " ('model', 1),\n",
              " ('net', 1),\n",
              " ('type', 1),\n",
              " ('anonymous', 1),\n",
              " ('occurring', 1),\n",
              " ('prints', 1),\n",
              " ('typical', 1),\n",
              " ('20', 1),\n",
              " ('afforded', 1),\n",
              " ('although', 1),\n",
              " ('commandline', 1),\n",
              " ('semistructured', 1),\n",
              " ('architecture1920', 1),\n",
              " ('minibatch', 1),\n",
              " ('done', 1),\n",
              " ('statistical', 1),\n",
              " ('api27', 1),\n",
              " ('deprecated', 1),\n",
              " ('formally', 1),\n",
              " ('mapreducestyle', 1),\n",
              " ('predecessor', 1),\n",
              " ('pregel', 1),\n",
              " ('utilized', 1),\n",
              " ('berkeleys', 1),\n",
              " ('research', 1),\n",
              " ('grunt', 1),\n",
              " ('marshalling', 1),\n",
              " ('massive', 1),\n",
              " ('banks', 1),\n",
              " ('from', 1),\n",
              " ('games', 1),\n",
              " ('giants', 1),\n",
              " ('some', 1),\n",
              " ('allocating', 1),\n",
              " ('normally', 1),\n",
              " ('workers', 1),\n",
              " ('founders', 1),\n",
              " ('if', 1),\n",
              " ('webbased', 1),\n",
              " ('advantages', 1),\n",
              " ('brought', 1),\n",
              " ('days', 1),\n",
              " ('prominence', 1),\n",
              " ('roadmap', 1),\n",
              " ('mapping', 1),\n",
              " ('particularly', 1),\n",
              " ('tend', 1),\n",
              " ('twostage', 1),\n",
              " ('speedup', 1),\n",
              " ('represents', 1),\n",
              " ('buckets', 1),\n",
              " ('created', 1),\n",
              " ('distributes', 1),\n",
              " ('processes', 1),\n",
              " ('borrowed', 1),\n",
              " ('packages', 1),\n",
              " ('popular', 1),\n",
              " ('selecting', 1),\n",
              " ('imported', 1),\n",
              " ('scalabased', 1),\n",
              " ('networks', 1),\n",
              " ('note', 1),\n",
              " ('review', 1),\n",
              " ('taking', 1),\n",
              " ('early', 1),\n",
              " ('resources', 1),\n",
              " ('everybody', 1),\n",
              " ('extended', 1),\n",
              " ('apex', 1),\n",
              " ('scenarios', 1),\n",
              " ('aggregations', 1),\n",
              " ('infinite', 1),\n",
              " ('live', 1),\n",
              " ('messages', 1),\n",
              " ('query', 1),\n",
              " ('23', 1),\n",
              " ('24', 1),\n",
              " ('low', 1),\n",
              " ('responses', 1),\n",
              " ('call', 1),\n",
              " ('tensorflow', 1),\n",
              " ('critical', 1),\n",
              " ('heitman', 1),\n",
              " ('lays', 1),\n",
              " ('api67', 1),\n",
              " ('faulttolerant', 1),\n",
              " ('followed', 1),\n",
              " ('machines', 1),\n",
              " ('multiset', 1),\n",
              " ('2012', 1),\n",
              " ('reduction', 1),\n",
              " ('algorithm', 1),\n",
              " ('interactiveexploratory', 1),\n",
              " ('loop', 1),\n",
              " ('repeated', 1),\n",
              " ('implementation29', 1),\n",
              " ('cpu', 1),\n",
              " ('kudu', 1),\n",
              " ('per', 1),\n",
              " ('scenario', 1),\n",
              " ('wide', 1),\n",
              " ('usable', 1),\n",
              " ('achieved', 1),\n",
              " ('mirrors', 1),\n",
              " ('passing', 1),\n",
              " ('produced', 1),\n",
              " ('style', 1),\n",
              " ('style2', 1),\n",
              " ('argument', 1),\n",
              " ('computes', 1),\n",
              " ('item', 1),\n",
              " ('rddcentric', 1),\n",
              " ('transform', 1),\n",
              " ('compiletime', 1),\n",
              " ('fully', 1),\n",
              " ('introduced', 1),\n",
              " ('manipulate', 1),\n",
              " ('cores', 1),\n",
              " ('enables', 1),\n",
              " ('convenience', 1),\n",
              " ('kinesis', 1),\n",
              " ('penalty', 1),\n",
              " ('center', 1),\n",
              " ('according', 1),\n",
              " ('gained', 1),\n",
              " ('scales', 1),\n",
              " ('wabbit24', 1),\n",
              " ('bagel', 1),\n",
              " ('general', 1),\n",
              " ('transactional', 1),\n",
              " ('updated', 1),\n",
              " ('mapreduce29', 1),\n",
              " ('viewed', 1),\n",
              " ('abstracts', 1),\n",
              " ('crunch', 1),\n",
              " ('worlds', 1),\n",
              " ('humble', 1),\n",
              " ('ibm', 1),\n",
              " ('ways', 1),\n",
              " ('assigned', 1),\n",
              " ('converts', 1),\n",
              " ('demand', 1),\n",
              " ('hdinsight', 1),\n",
              " ('acyclic', 1),\n",
              " ('user', 1),\n",
              " ('why', 1),\n",
              " ('old', 1),\n",
              " ('overtaking', 1),\n",
              " ('consisting', 1),\n",
              " ('creates', 1),\n",
              " ('efficiently', 1),\n",
              " ('memory', 1),\n",
              " ('multistage', 1),\n",
              " ('state', 1),\n",
              " ('developerfriendly', 1),\n",
              " ('50', 1),\n",
              " ('functionality', 1),\n",
              " ('providing', 1),\n",
              " ('fashion', 1),\n",
              " ('runs', 1),\n",
              " ('today', 1),\n",
              " ('alongside', 1),\n",
              " ('line', 1),\n",
              " ('bundles', 1),\n",
              " ('ease', 1),\n",
              " ('random', 1),\n",
              " ('saved', 1),\n",
              " ('neural', 1),\n",
              " ('selection', 1),\n",
              " ('structures', 1),\n",
              " ('addition', 1),\n",
              " ('domain', 1),\n",
              " ('helped', 1),\n",
              " ('involving', 1),\n",
              " ('kept', 1),\n",
              " ('leads', 1),\n",
              " ('operational', 1),\n",
              " ('previously', 1),\n",
              " ('requiring', 1),\n",
              " ('something', 1),\n",
              " ('sync', 1),\n",
              " ('traction', 1),\n",
              " ('developer', 1),\n",
              " ('share', 1),\n",
              " ('pure', 1),\n",
              " ('especially', 1),\n",
              " ('essentially', 1),\n",
              " ('interactive', 1),\n",
              " ('points', 1),\n",
              " ('streams', 1),\n",
              " ('experimental', 1),\n",
              " ('latencies', 1),\n",
              " ('scheme', 1),\n",
              " ('team', 1),\n",
              " ('bearable', 1),\n",
              " ('building', 1),\n",
              " ('continue', 1),\n",
              " ('legacy', 1),\n",
              " ('maintaining', 1),\n",
              " ('porting', 1),\n",
              " ('construct', 1),\n",
              " ('tutorials', 1),\n",
              " ('evan', 1),\n",
              " ('highly', 1),\n",
              " ('relatively', 1),\n",
              " ('science', 1),\n",
              " ('released', 1),\n",
              " ('though', 1),\n",
              " ('dataflow', 1),\n",
              " ('limitations', 1),\n",
              " ('deliberately', 1),\n",
              " ('facilitates', 1),\n",
              " ('databasestyle', 1),\n",
              " ('class', 1),\n",
              " ('magnitude', 1),\n",
              " ('install', 1),\n",
              " ('possible', 1),\n",
              " ('cassandra14', 1),\n",
              " ('mapr', 1),\n",
              " ('maprfs13', 1),\n",
              " ('pseudodistributed', 1),\n",
              " ('julia17', 1),\n",
              " ('faulttolerance', 1),\n",
              " ('keeping', 1),\n",
              " ('produce', 1),\n",
              " ('reconstructed', 1),\n",
              " ('broadcast', 1),\n",
              " ('imperative', 1),\n",
              " ('reductions', 1),\n",
              " ('flatmap', 1),\n",
              " ('pair', 1),\n",
              " ('reducebykey', 1),\n",
              " ('domainspecific', 1),\n",
              " ('odbcjdbc', 1),\n",
              " ('server', 1),\n",
              " ('strongly', 1),\n",
              " ('facilitating', 1),\n",
              " ('uses', 1),\n",
              " ('flink21', 1),\n",
              " ('flume', 1),\n",
              " ('alternating', 1),\n",
              " ('benchmarks', 1),\n",
              " ('better', 1),\n",
              " ('nine', 1),\n",
              " ('shipped', 1),\n",
              " ('graphprocessing', 1),\n",
              " ('alone', 1),\n",
              " ('edges', 1),\n",
              " ('massively', 1),\n",
              " ('property', 1),\n",
              " ('unsuitable', 1),\n",
              " ('initially', 1),\n",
              " ('easytouse', 1),\n",
              " ('quickly', 1),\n",
              " ('tandem', 1),\n",
              " ('bindings', 1),\n",
              " ('major', 1),\n",
              " ('tech', 1),\n",
              " ('mediate', 1),\n",
              " ('worker', 1),\n",
              " ('mean', 1),\n",
              " ('out', 1),\n",
              " ('want', 1),\n",
              " ('notebook', 1),\n",
              " ('misnomer', 1),\n",
              " ('5g', 1),\n",
              " ('cio', 1),\n",
              " ('download', 1),\n",
              " ('report', 1),\n",
              " ('back', 1),\n",
              " ('hundred', 1),\n",
              " ('means', 1),\n",
              " ('within', 1),\n",
              " ('second', 1),\n",
              " ('behind', 1),\n",
              " ('friendly', 1),\n",
              " ('hiding', 1),\n",
              " ('shown', 1),\n",
              " ('collection', 1),\n",
              " ('databases', 1),\n",
              " ('scaled', 1),\n",
              " ('bringing', 1),\n",
              " ('commonly', 1),\n",
              " ('known', 1),\n",
              " ('name', 1),\n",
              " ('pandas', 1),\n",
              " ('suggests', 1),\n",
              " ('connectors', 1),\n",
              " ('ecosystem', 1),\n",
              " ('jdbc', 1),\n",
              " ('others', 1),\n",
              " ('parquet', 1),\n",
              " ('reading', 1),\n",
              " ('applying', 1),\n",
              " ('feature', 1),\n",
              " ('javabased', 1),\n",
              " ('kmeans', 1),\n",
              " ('scientists', 1),\n",
              " ('trained', 1),\n",
              " ('details', 1),\n",
              " ('facilities', 1),\n",
              " ('infoworld', 1),\n",
              " ('graphframes', 1),\n",
              " ('despite', 1),\n",
              " ('obviously', 1),\n",
              " ('write', 1),\n",
              " ('manipulated', 1),\n",
              " ('mostly', 1),\n",
              " ('way', 1),\n",
              " ('wins', 1),\n",
              " ('flink', 1),\n",
              " ('all', 1),\n",
              " ('concerning', 1),\n",
              " ('eventtime', 1),\n",
              " ('real', 1),\n",
              " ('apply', 1),\n",
              " ('article', 1),\n",
              " ('guide', 1),\n",
              " ('guides', 1),\n",
              " ('increasingly', 1),\n",
              " ('learn', 1),\n",
              " ('neanderthal', 1),\n",
              " ('recommend', 1),\n",
              " ('scientist', 1),\n",
              " ('we', 1),\n",
              " ('codebase', 1),\n",
              " ('entire', 1),\n",
              " ('fault', 1),\n",
              " ('largescale', 1),\n",
              " ('opensource', 1),\n",
              " ('parallelism', 1),\n",
              " ('since', 1),\n",
              " ('tolerance', 1),\n",
              " ('1x', 1),\n",
              " ('architectural', 1),\n",
              " ('linear', 1),\n",
              " ('ms', 1),\n",
              " ('orders', 1),\n",
              " ('several', 1),\n",
              " ('systems', 1),\n",
              " ('daemons', 1),\n",
              " ('hdfs12', 1),\n",
              " ('lustre', 1),\n",
              " ('purposes', 1),\n",
              " ('usually', 1),\n",
              " ('cluster2', 1),\n",
              " ('functionalhigherorder', 1),\n",
              " ('invokes', 1),\n",
              " ('joins', 1),\n",
              " ('loss', 1),\n",
              " ('schedules', 1),\n",
              " ('track', 1),\n",
              " ('forms', 1),\n",
              " ('applies', 1),\n",
              " ('operation', 1),\n",
              " ('dataframesa', 1),\n",
              " ('lack', 1),\n",
              " ('ingests', 1),\n",
              " ('consume', 1),\n",
              " ('duration', 1),\n",
              " ('equal', 1),\n",
              " ('kafka', 1),\n",
              " ('sockets22', 1),\n",
              " ('tcpip', 1),\n",
              " ('twitter', 1),\n",
              " ('streaming23', 1),\n",
              " ('onpremises', 1),\n",
              " ('least', 1),\n",
              " ('memorybased', 1),\n",
              " ('simplifies', 1),\n",
              " ('squares', 1),\n",
              " ('because', 1),\n",
              " ('full', 1),\n",
              " ('unlike', 1),\n",
              " ('vertices28', 1),\n",
              " ('started', 1),\n",
              " ('burdens', 1),\n",
              " ('computers', 1),\n",
              " ('shoulders', 1),\n",
              " ('2009', 1),\n",
              " ('governments', 1),\n",
              " ('telecommunications', 1),\n",
              " ('execute', 1),\n",
              " ('fundamental', 1),\n",
              " ('main', 1),\n",
              " ('care', 1),\n",
              " ('cloudera', 1),\n",
              " ('hortonworks', 1),\n",
              " ('resource', 1),\n",
              " ('employs', 1),\n",
              " ('integrated', 1),\n",
              " ('commands', 1),\n",
              " ('layer', 1),\n",
              " ('choice', 1),\n",
              " ('included', 1),\n",
              " ('pointing', 1),\n",
              " ('10', 1),\n",
              " ('contained', 1),\n",
              " ('essence', 1),\n",
              " ('first', 1),\n",
              " ('situations', 1),\n",
              " ('speed', 1),\n",
              " ('argue', 1),\n",
              " ('friendliness', 1),\n",
              " ('almost', 1),\n",
              " ('calls', 1),\n",
              " ('count', 1),\n",
              " ('aggregation', 1),\n",
              " ('nosql', 1),\n",
              " ('combining', 1),\n",
              " ('splits', 1),\n",
              " ('focused', 1),\n",
              " ('datastores', 1),\n",
              " ('hbase', 1),\n",
              " ('json', 1),\n",
              " ('orc', 1),\n",
              " ('extraction', 1),\n",
              " ('forests', 1),\n",
              " ('includes', 1),\n",
              " ('production', 1),\n",
              " ('selections', 1),\n",
              " ('techniques', 1),\n",
              " ('regression', 1),\n",
              " ('see', 1),\n",
              " ('concerns', 1),\n",
              " ('disparate', 1),\n",
              " ('environments', 1),\n",
              " ('things', 1),\n",
              " ('breaking', 1),\n",
              " ('operator', 1),\n",
              " ('series', 1),\n",
              " ('able', 1),\n",
              " ('criticism', 1),\n",
              " ('match', 1),\n",
              " ('create', 1),\n",
              " ('dealing', 1),\n",
              " ('delivery', 1),\n",
              " ('earlier', 1),\n",
              " ('pain', 1),\n",
              " ('struggled', 1),\n",
              " ('1ms', 1),\n",
              " ('considered', 1),\n",
              " ('handle', 1),\n",
              " ('relied', 1),\n",
              " ('while', 1),\n",
              " ('lot', 1),\n",
              " ('recommends', 1),\n",
              " ('keras', 1),\n",
              " ('lowerlevel', 1),\n",
              " ('statements', 1),\n",
              " ('udfs', 1),\n",
              " ('via', 1),\n",
              " ('perspective', 1),\n",
              " ('ready', 1),\n",
              " ('sense', 1),\n",
              " ('terms', 1)]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(result.items(), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OViND9SFFTOd"
      },
      "source": [
        "Да, было бы проще все сделать иным кодом и в один проход, но целью было разобрать, как все это примерно работает под капотом на больших данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhK08F7hFjv_"
      },
      "source": [
        "**Домашнее задание**\n",
        "\n",
        "Посчитать количество рейтингов больше 4 для каждого фильма и вывести фильмы в порядке убывания количества этих оценок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sPx9_0-wFlK-"
      },
      "outputs": [],
      "source": [
        "with open('user_ratedmovies.dat', 'rb') as f:\n",
        "    data = f.readlines()\n",
        "headers = data[0].decode().split('\\t')[:3]\n",
        "data = [row.decode().split('\\t')[:3] for row in data[1:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8yWQm0zJegy",
        "outputId": "fa18875c-ae8a-4f47-f000-9e4222696f6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['userID', 'movieID', 'rating']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgDpQdAeRG5e",
        "outputId": "533893f8-7190-441e-9844-87ea93d113f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['75', '3', '1']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln4GFhQZH5tM",
        "outputId": "e18c9ff8-16e2-408b-f6b2-51d91de24c0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "855598"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzkqhk9MRL8P"
      },
      "source": [
        "Пишем map, shiffle и reduce + параллелим вычисления. Лучше задавать batch_size при распараллеливании, либо даже заранее все разбить на батчи, будет быстрее\n",
        "\n",
        "Также посмотрите на то, нет ли перекоса в данных после shuffle, можете попробовать использовать остаток от деления не простого hash, а ввести какую-то функию"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAr0OBzMro3W"
      },
      "source": [
        "* Представлю, что у меня 20 Data Node и все работают, тогда поделю всю выборку на 20 батчей, каждый из которых будет храниться и мапиться на своей ноде"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "_1aUiwrJRK3W"
      },
      "outputs": [],
      "source": [
        "def map_rating(film_with_rating):\n",
        "\n",
        "    film_with_rating = (int(film_with_rating[1]), (1 if float(film_with_rating[2])>4 else 0))\n",
        "    return film_with_rating\n",
        "\n",
        "def create_chunks(shuffled_data):\n",
        "    result = {}\n",
        "    for idx, data in shuffled_data:\n",
        "        if idx in result:\n",
        "            result[idx].append(data)\n",
        "        else:\n",
        "            result[idx] = [data]\n",
        "    return list(result.items())\n",
        "\n",
        "def shuffle_rating(mapper_result, n_nodes=5):\n",
        "    # shuffled_data = []\n",
        "    # for key, value in mapper_result:\n",
        "    #     shuffled_data.append((hash(key)%n_nodes, (key, value)))\n",
        "    # shuffled_data = sorted(shuffled_data, key=lambda x: x[0])\n",
        "    # chunks = create_chunks(shuffled_data)\n",
        "    return (hash(mapper_result[0])%n_nodes, (mapper_result[0], mapper_result[1]))\n",
        "\n",
        "def reduce_rating(values_to_reduce):\n",
        "    result = {}\n",
        "    for key, value in values_to_reduce:\n",
        "        if key in result:\n",
        "            result[key] += value\n",
        "        else:\n",
        "            result[key] = value\n",
        "    return result\n",
        "\n",
        "def map_shuffle(data, n_nodes):\n",
        "    map_stage = map_rating(data)\n",
        "    shuffle_stage = shuffle_rating(map_stage, n_nodes)\n",
        "    return shuffle_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEKRuYrRro3W",
        "outputId": "2d3b617f-3874-4277-8f2b-a4899aa27c46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['75', '3', '1'], (3, 0))"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "data[0], map_rating(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-4aQUE5ro3W",
        "outputId": "1b174b69-bec1-4487-c694-12df5928f84d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3, (3, 0)), (4, (904, 1)))"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "shuffle_rating(map_rating(data[0])), shuffle_rating(map_rating(data[100]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MUqQEftro3W",
        "outputId": "aa14b2ee-dac3-4dd7-f0ca-14e2b4f90894"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, (3, 0))"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "map_shuffle(data[0], 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "KJdLsomDRyS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc68d6e-2cf7-48b7-a1b4-9a2dab0059fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
            "[Parallel(n_jobs=20)]: Done  37 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=20)]: Done 253 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=20)]: Done 613 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=20)]: Done 1117 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=20)]: Done 1765 tasks      | elapsed:    3.6s\n",
            "[Parallel(n_jobs=20)]: Done 2557 tasks      | elapsed:    4.4s\n",
            "[Parallel(n_jobs=20)]: Done 3493 tasks      | elapsed:    5.5s\n",
            "[Parallel(n_jobs=20)]: Done 4573 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=20)]: Done 5797 tasks      | elapsed:    7.3s\n",
            "[Parallel(n_jobs=20)]: Done 7165 tasks      | elapsed:    8.3s\n",
            "[Parallel(n_jobs=20)]: Done 8677 tasks      | elapsed:    9.4s\n",
            "[Parallel(n_jobs=20)]: Done 10333 tasks      | elapsed:   10.6s\n",
            "[Parallel(n_jobs=20)]: Done 12133 tasks      | elapsed:   12.0s\n",
            "[Parallel(n_jobs=20)]: Done 14077 tasks      | elapsed:   13.4s\n",
            "[Parallel(n_jobs=20)]: Done 16165 tasks      | elapsed:   14.9s\n",
            "[Parallel(n_jobs=20)]: Done 18397 tasks      | elapsed:   17.0s\n",
            "[Parallel(n_jobs=20)]: Done 20773 tasks      | elapsed:   19.6s\n",
            "[Parallel(n_jobs=20)]: Done 23293 tasks      | elapsed:   21.7s\n",
            "[Parallel(n_jobs=20)]: Done 25957 tasks      | elapsed:   24.3s\n",
            "[Parallel(n_jobs=20)]: Done 28765 tasks      | elapsed:   27.4s\n",
            "[Parallel(n_jobs=20)]: Done 31717 tasks      | elapsed:   29.6s\n",
            "[Parallel(n_jobs=20)]: Done 34813 tasks      | elapsed:   32.7s\n",
            "[Parallel(n_jobs=20)]: Done 38053 tasks      | elapsed:   35.9s\n",
            "[Parallel(n_jobs=20)]: Done 41437 tasks      | elapsed:   38.4s\n",
            "[Parallel(n_jobs=20)]: Done 44965 tasks      | elapsed:   40.9s\n",
            "[Parallel(n_jobs=20)]: Done 48637 tasks      | elapsed:   43.7s\n",
            "[Parallel(n_jobs=20)]: Done 52453 tasks      | elapsed:   47.4s\n",
            "[Parallel(n_jobs=20)]: Done 56413 tasks      | elapsed:   51.1s\n",
            "[Parallel(n_jobs=20)]: Done 60517 tasks      | elapsed:   54.1s\n",
            "[Parallel(n_jobs=20)]: Done 64765 tasks      | elapsed:   57.1s\n",
            "[Parallel(n_jobs=20)]: Done 69157 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=20)]: Done 73693 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=20)]: Done 78373 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=20)]: Done 83197 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=20)]: Done 88165 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=20)]: Done 93277 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=20)]: Done 98533 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=20)]: Done 103933 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=20)]: Done 109477 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=20)]: Done 115165 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=20)]: Done 120997 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=20)]: Done 126973 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=20)]: Done 133093 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=20)]: Done 139357 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=20)]: Done 145765 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=20)]: Done 152317 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=20)]: Done 159013 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=20)]: Done 165853 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=20)]: Done 172837 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=20)]: Done 179965 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=20)]: Done 187237 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=20)]: Done 194653 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=20)]: Done 202213 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=20)]: Done 209917 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=20)]: Done 217765 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=20)]: Done 225757 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=20)]: Done 233893 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=20)]: Done 242173 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=20)]: Done 250597 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=20)]: Done 259165 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=20)]: Done 267877 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=20)]: Done 276733 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=20)]: Done 285733 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=20)]: Done 294877 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=20)]: Done 304165 tasks      | elapsed:  4.4min\n",
            "[Parallel(n_jobs=20)]: Done 313597 tasks      | elapsed:  4.5min\n",
            "[Parallel(n_jobs=20)]: Done 323173 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=20)]: Done 332893 tasks      | elapsed:  4.8min\n",
            "[Parallel(n_jobs=20)]: Done 342757 tasks      | elapsed:  4.9min\n",
            "[Parallel(n_jobs=20)]: Done 352765 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=20)]: Done 362917 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=20)]: Done 373213 tasks      | elapsed:  5.4min\n",
            "[Parallel(n_jobs=20)]: Done 383653 tasks      | elapsed:  5.5min\n",
            "[Parallel(n_jobs=20)]: Done 394237 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=20)]: Done 404965 tasks      | elapsed:  5.8min\n",
            "[Parallel(n_jobs=20)]: Done 415837 tasks      | elapsed:  6.0min\n",
            "[Parallel(n_jobs=20)]: Done 426853 tasks      | elapsed:  6.1min\n",
            "[Parallel(n_jobs=20)]: Done 438013 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=20)]: Done 449317 tasks      | elapsed:  6.4min\n",
            "[Parallel(n_jobs=20)]: Done 460765 tasks      | elapsed:  6.6min\n",
            "[Parallel(n_jobs=20)]: Done 472357 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=20)]: Done 484093 tasks      | elapsed:  6.9min\n",
            "[Parallel(n_jobs=20)]: Done 495973 tasks      | elapsed:  7.1min\n",
            "[Parallel(n_jobs=20)]: Done 507997 tasks      | elapsed:  7.3min\n",
            "[Parallel(n_jobs=20)]: Done 520165 tasks      | elapsed:  7.4min\n",
            "[Parallel(n_jobs=20)]: Done 532477 tasks      | elapsed:  7.6min\n",
            "[Parallel(n_jobs=20)]: Done 544933 tasks      | elapsed:  7.8min\n",
            "[Parallel(n_jobs=20)]: Done 557533 tasks      | elapsed:  8.0min\n",
            "[Parallel(n_jobs=20)]: Done 570277 tasks      | elapsed:  8.1min\n",
            "[Parallel(n_jobs=20)]: Done 583165 tasks      | elapsed:  8.3min\n",
            "[Parallel(n_jobs=20)]: Done 596197 tasks      | elapsed:  8.5min\n",
            "[Parallel(n_jobs=20)]: Done 609373 tasks      | elapsed:  8.7min\n",
            "[Parallel(n_jobs=20)]: Done 622693 tasks      | elapsed:  8.9min\n",
            "[Parallel(n_jobs=20)]: Done 636157 tasks      | elapsed:  9.0min\n",
            "[Parallel(n_jobs=20)]: Done 649765 tasks      | elapsed:  9.2min\n",
            "[Parallel(n_jobs=20)]: Done 663517 tasks      | elapsed:  9.4min\n",
            "[Parallel(n_jobs=20)]: Done 677413 tasks      | elapsed:  9.6min\n",
            "[Parallel(n_jobs=20)]: Done 691453 tasks      | elapsed:  9.8min\n",
            "[Parallel(n_jobs=20)]: Done 705637 tasks      | elapsed: 10.0min\n",
            "[Parallel(n_jobs=20)]: Done 719965 tasks      | elapsed: 10.2min\n",
            "[Parallel(n_jobs=20)]: Done 734437 tasks      | elapsed: 10.5min\n",
            "[Parallel(n_jobs=20)]: Done 749053 tasks      | elapsed: 10.6min\n",
            "[Parallel(n_jobs=20)]: Done 763813 tasks      | elapsed: 10.8min\n",
            "[Parallel(n_jobs=20)]: Done 778717 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=20)]: Done 793765 tasks      | elapsed: 11.3min\n",
            "[Parallel(n_jobs=20)]: Done 808957 tasks      | elapsed: 11.5min\n",
            "[Parallel(n_jobs=20)]: Done 824293 tasks      | elapsed: 11.7min\n",
            "[Parallel(n_jobs=20)]: Done 839773 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=20)]: Done 855397 tasks      | elapsed: 12.1min\n",
            "[Parallel(n_jobs=20)]: Done 855598 out of 855598 | elapsed: 12.1min finished\n"
          ]
        }
      ],
      "source": [
        "with Parallel(n_jobs=20, verbose=-1, batch_size=1) as parallel:\n",
        "    res = parallel(delayed(map_shuffle)(df, 5) for df in data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res[:10]"
      ],
      "metadata": {
        "id": "VgFwOuLd4e04",
        "outputId": "1a820277-8e2e-4099-af01-7b7cc40426e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, (3, 0)),\n",
              " (2, (32, 1)),\n",
              " (0, (110, 0)),\n",
              " (0, (160, 0)),\n",
              " (3, (163, 0)),\n",
              " (0, (165, 1)),\n",
              " (3, (173, 0)),\n",
              " (1, (296, 1)),\n",
              " (3, (353, 0)),\n",
              " (0, (420, 0))]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "9uOmGh6hro3W"
      },
      "outputs": [],
      "source": [
        "#перегоняю все на 5 нод для reduce на каждой\n",
        "shuffle_stage = {i:[] for i in range(5)}\n",
        "for key, value in res:\n",
        "    shuffle_stage[key].append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYm8bvPxro3W",
        "outputId": "e107fc23-609b-43c6-f052-b4507bee7629"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "# данные после мапа разлетелись на 5 нод для reduce\n",
        "len(shuffle_stage)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_stage[0][:10]"
      ],
      "metadata": {
        "id": "VDklpuiM75oy",
        "outputId": "2f105dff-f78a-4b0f-bafa-e5d559be3bc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(110, 0),\n",
              " (160, 0),\n",
              " (165, 1),\n",
              " (420, 0),\n",
              " (920, 0),\n",
              " (1215, 1),\n",
              " (1370, 0),\n",
              " (1485, 0),\n",
              " (2490, 0),\n",
              " (2640, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95Tk-kEVro3W",
        "outputId": "1d57de87-6972-4170-b643-73781fcb11e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: number of rows = 170211\n",
            "1: number of rows = 169817\n",
            "2: number of rows = 170990\n",
            "3: number of rows = 183481\n",
            "4: number of rows = 161099\n"
          ]
        }
      ],
      "source": [
        "for key in shuffle_stage.keys():\n",
        "    print(f'{key}: number of rows = {len(shuffle_stage[key])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "SmKhm2eeRyVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd3005d-f31c-46de-c5e9-803200c45eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.4s remaining:    0.6s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.5s remaining:    0.3s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.7s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.7s finished\n"
          ]
        }
      ],
      "source": [
        "with Parallel(n_jobs=5, verbose=10, batch_size=1) as parallel:\n",
        "    res = parallel(delayed(reduce_rating)(shuffle_stage[key]) for key in shuffle_stage.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR_nesQrro3W",
        "outputId": "767f73d5-dd19-46f1-b1b8-acab03027a75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "len(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "6Gis5YMgro3X"
      },
      "outputs": [],
      "source": [
        "# Соедиеняем все выходы reduce для итогового результата\n",
        "total_res = {}\n",
        "\n",
        "for r in res:\n",
        "    total_res.update(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11D-vy7Rro3X",
        "outputId": "043fc303-af3b-4288-9315-91c361e908e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2571, 900),\n",
              " (318, 887),\n",
              " (296, 878),\n",
              " (2959, 828),\n",
              " (4993, 756),\n",
              " (7153, 719),\n",
              " (5952, 697),\n",
              " (858, 690),\n",
              " (50, 688),\n",
              " (2858, 680)]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "sorted(total_res.items(), key=lambda item: item[1], reverse=True)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "MnTXdMs3ro3X",
        "outputId": "0fa2a8e2-5267-4290-b700-15ec4eae0bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-139-8a6dd900f7cc>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  pd.DataFrame(columns=headers, data=np.array(data).astype(np.float)).query(\"rating>4\")\\\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         userID  rating\n",
              "movieID                \n",
              "2571.0      900     900\n",
              "318.0       887     887\n",
              "296.0       878     878\n",
              "2959.0      828     828\n",
              "4993.0      756     756\n",
              "7153.0      719     719\n",
              "5952.0      697     697\n",
              "858.0       690     690\n",
              "50.0        688     688\n",
              "2858.0      680     680"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c74d1af-3b97-41f2-a9cf-3535f84b76a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>movieID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2571.0</th>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318.0</th>\n",
              "      <td>887</td>\n",
              "      <td>887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296.0</th>\n",
              "      <td>878</td>\n",
              "      <td>878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2959.0</th>\n",
              "      <td>828</td>\n",
              "      <td>828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993.0</th>\n",
              "      <td>756</td>\n",
              "      <td>756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7153.0</th>\n",
              "      <td>719</td>\n",
              "      <td>719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5952.0</th>\n",
              "      <td>697</td>\n",
              "      <td>697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858.0</th>\n",
              "      <td>690</td>\n",
              "      <td>690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50.0</th>\n",
              "      <td>688</td>\n",
              "      <td>688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2858.0</th>\n",
              "      <td>680</td>\n",
              "      <td>680</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c74d1af-3b97-41f2-a9cf-3535f84b76a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c74d1af-3b97-41f2-a9cf-3535f84b76a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c74d1af-3b97-41f2-a9cf-3535f84b76a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.DataFrame(columns=headers, data=np.array(data).astype(np.float)).query(\"rating>4\")\\\n",
        "                                    .groupby('movieID').count()\\\n",
        "                                    .sort_values(by='rating', ascending=False)\\\n",
        "                                    .head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNFLhhsKro3X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}